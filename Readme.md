ğŸš€ My GenAI Learning Journey (Google Gemini)

This repository documents my day-by-day journey of learning Generative AI using Google Gemini (FREE).
I am learning GenAI from scratch, focusing on fundamentals â†’ real applications, using Python.

This README will be updated daily as I progress.


Day 1 â€“ GenAI Setup & Prompt Basics (Google Gemini)
What I Learned Today

Today I started my GenAI journey using Google Gemini (FREE) and learned the basics required to interact with a Generative AI model using Python.

ğŸ”¹ Topics Covered

What is Generative AI
What is a Prompt
Google Gemini API setup
Text generation using Gemini
Temperature parameter
Max output tokens
Prompt engineering basics
PromptTemplate using LangChain

âš™ï¸ Environment Setup
1ï¸âƒ£ Create Virtual Environment
python -m venv genai_env
genai_env\Scripts\activate

2ï¸âƒ£ Install Required Libraries
pip install google-generativeai langchain langchain-google-genai python-dotenv

3ï¸âƒ£ API Key Setup

Created .env file:

GOOGLE_API_KEY=your_api_key_here

ğŸ”‘ Gemini Configuration (Python)
import google.generativeai as genai
import os
from dotenv import load_dotenv

load_dotenv()
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

âœ¨ Text Generation Using Gemini
model = genai.GenerativeModel("gemini-pro")
response = model.generate_content("Explain Django in simple words")
print(response.text)

ğŸ§  What is a Prompt?

A prompt is the instruction or input given to the AI to generate a response.

Example:
Explain Django ORM for beginners in simple Hinglish using 5 bullet points.

ğŸŒ¡ï¸ Temperature Parameter

Temperature controls randomness of AI responses.

model = genai.GenerativeModel(
    "gemini-pro",
    generation_config={"temperature": 0.3}
)


Low temperature â†’ factual answers

High temperature â†’ creative answers

ğŸ“ Max Output Tokens

Controls the length of the response.

model = genai.GenerativeModel(
    "gemini-pro",
    generation_config={
        "temperature": 0.3,
        "max_output_tokens": 150
    }
)

ğŸ”— Using Gemini with LangChain
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-pro",
    temperature=0.3
)

print(llm.invoke("What is REST API?").content)

ğŸ“¦ PromptTemplate (Reusable Prompts)
from langchain.prompts import PromptTemplate

template = PromptTemplate(
    input_variables=["topic"],
    template="""
    You are a teacher.
    Explain {topic} in simple Hinglish.
    Give only 5 bullet points.
    """
)

prompt = template.format(topic="Django ORM")
print(llm.invoke(prompt).content)



Day 2 â€“ Learned BAout Tokens

1ï¸âƒ£ Tokens (LangChain Context)
What are Tokens?

Tokens are the smallest text units processed by the LLM connected through LangChain.
Input tokens â†’ your prompt + chat history
Output tokens â†’ AI response
LangChain sends both to the model

Example:

"I love Django"
â‰ˆ 3â€“4 tokens

Max Output Tokens in LangChain

In LangChain, max_output_tokens limits how many tokens the model can generate.

llm = ChatGoogleGenerativeAI(
    model="gemini-pro",
    temperature=0.3,
    max_output_tokens=150
)

Important Behavior âš ï¸

Gemini does not pre-plan the response
It generates text continuously
When token limit is reached â†’ response is truncated (cut)

ğŸ‘‰ Low token limit can cause:

Incomplete sentences
Broken explanations
Poor user experience




Day 3 â€“ Chaining in LangChain (LCEL)
ğŸ“Œ Topic: Chaining & Its Types (Sequential & Parallel)

Today I learned about chaining in LangChain using LCEL (LangChain Expression Language).
Chaining is one of the core concepts used to build complex GenAI workflows.

ğŸ”¹ What is Chaining?

Chaining means connecting multiple components like:
PromptTemplate
LLM (Google Gemini)
Output Parser
so that the output of one step becomes the input of the next step automatically.

In LangChain, this is done using the pipe (|) operator, which is part of LCEL.

Prompt â†’ Model â†’ Parser â†’ Next Prompt â†’ Model

ğŸ”¹ LCEL (LangChain Expression Language)
LCEL is a declarative way to build chains using runnable components.

Example:
chain = prompt | llm | StrOutputParser()

Here:
prompt creates input
llm generates output
parser converts output to string

ğŸ”¹ Types of Chaining Learned Today
1ï¸âƒ£ Sequential Chaining

In Sequential Chaining,
the output of one step is passed step-by-step to the next component.

Example Use Case:

Generate text
Summarize that text
Refine the summary

Flow:
Step 1 â†’ Step 2 â†’ Step 3

Example:
chain = prompt1 | llm | parser | prompt2 | llm | parser


âœ” Output flows linearly
âœ” Best for multi-step reasoning tasks

2ï¸âƒ£ Parallel Chaining

In Parallel Chaining,
the same input is processed by multiple chains at the same time,
and the outputs are collected in a dictionary.

This is done using RunnableParallel.

Example Use Case:

Generate notes
Generate quiz
Merge both later

Flow:
           â†’ Chain A (Notes)
Input â†’
           â†’ Chain B (Quiz)

Example:
parallel_chain = RunnableParallel({
    "notes": prompt1 | llm | parser,
    "quiz":  prompt2 | llm | parser
})


âœ” Runs chains simultaneously
âœ” Output format:

{
  "notes": "...",
  "quiz": "..."
}

ğŸ”¹ Important Learnings

StrOutputParser converts LLM output into a single string
RunnableParallel collects multiple outputs into a dictionary
Parser does not store variables, it only transforms output
LCEL automatically passes data between components
Prompt variable names must match input keys exactly


Note: You can check Code in Chain.py file